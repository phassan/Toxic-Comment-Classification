{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfELMfiF2nsM"
   },
   "source": [
    "## Bidirectional LSTM for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WC_VxUNL2m_q",
    "outputId": "6af2beaa-46d0-4c30-87f2-a26e0477587b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NdyD9Ad2m4Q"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgtAGCsC2mxV"
   },
   "outputs": [],
   "source": [
    "# Reads in both training and testing dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "OBO1qWVf2mOE",
    "outputId": "feaf082e-00ab-49ac-f8fb-b7e6275a3d40"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "N5NRJl3u2mKJ",
    "outputId": "67aa5bde-b970-4df4-b0c2-0a15b7867a73"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BImQbQ7S4xXt",
    "outputId": "57ff85fd-f3f8-4e12-dfc4-c4dd4487db5f"
   },
   "outputs": [],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivM9LtXw4xPn"
   },
   "outputs": [],
   "source": [
    "contractions_dict = {\"ain't\": \"is not\",\n",
    "                    \"aren't\": \"are not\",\n",
    "                    \"can't\": \"cannot\", \n",
    "                    \"can't've\": \"cannot have\", \n",
    "                    \"'cause\": \"because\", \n",
    "                    \"could've\": \"could have\", \n",
    "                    \"couldn't\": \"could not\", \n",
    "                    \"couldn't've\": \"could not have\",\n",
    "                    \"didn't\": \"did not\", \n",
    "                    \"doesn't\": \"does not\", \n",
    "                    \"don't\": \"do not\", \n",
    "                    \"hadn't\": \"had not\",  \n",
    "                    \"hadn't've\": \"had not have\", \n",
    "                    \"hasn't\": \"has not\", \n",
    "                    \"haven't\": \"have not\", \n",
    "                    \"he'd\": \"he would\", \n",
    "                    \"he'd've\": \"he would have\", \n",
    "                    \"he'll\": \"he will\", \n",
    "                    \"he'll've\": \"he will have\", \n",
    "                    \"he's\": \"he is\", \n",
    "                    \"how'd\": \"how did\", \n",
    "                    \"how'd'y\": \"how do you\", \n",
    "                    \"how'll\": \"how will\", \n",
    "                    \"how's\": \"how is\", \n",
    "                    \"I'd\": \"I would\", \n",
    "                    \"I'd've\": \"I would have\", \n",
    "                    \"I'll\": \"I will\", \n",
    "                    \"I'll've\": \"I will have\",\n",
    "                    \"I'm\": \"I am\", \n",
    "                    \"I've\": \"I have\", \n",
    "                    \"i'd\": \"i would\", \n",
    "                    \"i'd've\": \"i would have\", \n",
    "                    \"i'll\": \"i will\", \n",
    "                    \"i'll've\": \"i will have\",\n",
    "                    \"i'm\": \"i am\", \n",
    "                    \"i've\": \"i have\", \n",
    "                    \"isn't\": \"is not\", \n",
    "                    \"it'd\": \"it would\", \n",
    "                    \"it'd've\": \"it would have\", \n",
    "                    \"it'll\": \"it will\", \n",
    "                    \"it'll've\": \"it will have\",\n",
    "                    \"it's\": \"it is\", \n",
    "                    \"let's\": \"let us\", \n",
    "                    \"ma'am\": \"madam\", \n",
    "                    \"mayn't\": \"may not\", \n",
    "                    \"might've\": \"might have\",\n",
    "                    \"mightn't\": \"might not\",\n",
    "                    \"mightn't've\": \"might not have\", \n",
    "                    \"must've\": \"must have\", \n",
    "                    \"mustn't\": \"must not\", \n",
    "                    \"mustn't've\": \"must not have\", \n",
    "                    \"needn't\": \"need not\", \n",
    "                    \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \n",
    "                    \"oughtn't\": \"ought not\", \n",
    "                    \"oughtn't've\": \"ought not have\", \n",
    "                    \"shan't\": \"shall not\",\n",
    "                    \"sha'n't\": \"shall not\", \n",
    "                    \"shan't've\": \"shall not have\", \n",
    "                    \"she'd\": \"she would\", \n",
    "                    \"she'd've\": \"she would have\", \n",
    "                    \"she'll\": \"she will\", \n",
    "                    \"she'll've\": \"she will have\", \n",
    "                    \"she's\": \"she is\", \n",
    "                    \"should've\": \"should have\", \n",
    "                    \"shouldn't\": \"should not\", \n",
    "                    \"shouldn't've\": \"should not have\", \n",
    "                    \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \n",
    "                    \"this's\": \"this is\",\n",
    "                    \"that'd\": \"that would\", \n",
    "                    \"that'd've\": \"that would have\",\n",
    "                    \"that's\": \"that is\", \n",
    "                    \"there'd\": \"there would\", \n",
    "                    \"there'd've\": \"there would have\",\n",
    "                    \"there's\": \"there is\", \n",
    "                    \"here's\": \"here is\",\n",
    "                    \"they'd\": \"they would\", \n",
    "                    \"they'd've\": \"they would have\", \n",
    "                    \"they'll\": \"they will\", \n",
    "                    \"they'll've\": \"they will have\", \n",
    "                    \"they're\": \"they are\", \n",
    "                    \"they've\": \"they have\",\n",
    "                    \"to've\": \"to have\", \n",
    "                    \"wasn't\": \"was not\", \n",
    "                    \"we'd\": \"we would\", \n",
    "                    \"we'd've\": \"we would have\", \n",
    "                    \"we'll\": \"we will\", \n",
    "                    \"we'll've\": \"we will have\", \n",
    "                    \"we're\": \"we are\", \n",
    "                    \"we've\": \"we have\", \n",
    "                    \"weren't\": \"were not\",\n",
    "                    \"what'll\": \"what will\", \n",
    "                    \"what'll've\": \"what will have\", \n",
    "                    \"what're\": \"what are\", \n",
    "                    \"what's\": \"what is\", \n",
    "                    \"what've\": \"what have\", \n",
    "                    \"when's\": \"when is\", \n",
    "                    \"when've\": \"when have\", \n",
    "                    \"where'd\": \"where did\", \n",
    "                    \"where's\": \"where is\", \n",
    "                    \"where've\": \"where have\", \n",
    "                    \"who'll\": \"who will\", \n",
    "                    \"who'll've\": \"who will have\", \n",
    "                    \"who's\": \"who is\", \n",
    "                    \"who've\": \"who have\", \n",
    "                    \"why's\": \"why is\", \n",
    "                    \"why've\": \"why have\", \n",
    "                    \"will've\": \"will have\", \n",
    "                    \"won't\": \"will not\",\n",
    "                    \"won't've\": \"will not have\", \n",
    "                    \"would've\": \"would have\", \n",
    "                    \"wouldn't\": \"would not\", \n",
    "                    \"wouldn't've\": \"would not have\", \n",
    "                    \"y'all\": \"you all\", \n",
    "                    \"y'all'd\": \"you all would\",\n",
    "                    \"y'all'd've\": \"you all would have\",\n",
    "                    \"y'all're\": \"you all are\",\n",
    "                    \"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \n",
    "                    \"you'd've\": \"you would have\", \n",
    "                    \"you'll\": \"you will\", \n",
    "                    \"you'll've\": \"you will have\", \n",
    "                    \"you're\": \"you are\", \n",
    "                    \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbDPHJEW4xID"
   },
   "outputs": [],
   "source": [
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    for key in contractions_dict:\n",
    "        value = contractions_dict[key]\n",
    "        s = s.replace(key, value)   \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buYaABRU4w-6"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(s):\n",
    "        s = str(s)\n",
    "        s = s.lower()\n",
    "        s = expand_contractions(s)\n",
    "        s = re.sub(\"\\n\",\" \",s)\n",
    "        s = re.sub(\"\\[.*\\]\",\" \",s)\n",
    "        s = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\" \",s)\n",
    "        s = re.sub(r\"\\?\",\" \",s)\n",
    "        s = re.sub(r'[^\\w\\s]','',s)\n",
    "        s = re.sub(\"\\d+\", \"\", s)\n",
    "        s = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', s) \n",
    "        s = re.sub(r'(\\W)\\1+', r'\\1', s) \n",
    "        \n",
    "        words = word_tokenize(s)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words_lemma = []\n",
    "        for word, tag in pos_tag(words):\n",
    "            if tag.startswith(\"NN\"):\n",
    "                words_lemma.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "            elif tag.startswith('VB'):\n",
    "                words_lemma.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "            elif tag.startswith('JJ'):\n",
    "                words_lemma.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "            elif tag.startswith('R'):\n",
    "                words_lemma.append(lemmatizer.lemmatize(word, pos='r'))    \n",
    "            else:\n",
    "                words_lemma.append(word)\n",
    "\n",
    "        return (\" \".join(words_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "yLQ5zGDn4w03",
    "outputId": "fd4c69cd-e1e7-4050-f37c-c4ef2a2e7264"
   },
   "outputs": [],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "train[\"comment_text\"] = train[\"comment_text\"].fillna(\" \")\n",
    "test[\"comment_text\"] = test[\"comment_text\"].fillna(\" \")\n",
    "train['clean_comment_text'] = train['comment_text'].map(lambda x: text_cleaner(x))\n",
    "test['clean_comment_text'] = test['comment_text'].map(lambda x: text_cleaner(x))\n",
    "\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Tokenizing using Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVBxSU4p4woq"
   },
   "outputs": [],
   "source": [
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "TRUNC_TYPE='post'\n",
    "PADDING_TYPE='post'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train['clean_comment_text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train['clean_comment_text'])\n",
    "train_padded_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test['clean_comment_text'])\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=PADDING_TYPE, truncating=TRUNC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bYBMIWST5mZW",
    "outputId": "877540d3-70f1-4720-b00e-ebd0965e64ad"
   },
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open('glove.840B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ' '.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embedding_index[word] = coefs.reshape(-1)\n",
    "        coef = embedding_index[word]\n",
    "print('Total word vectors: %s' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G13KsreE5mN5"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((vocab_size+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AZ5H8vja5mBw",
    "outputId": "6434a6af-4b03-431c-e27a-d2ea738d5879"
   },
   "outputs": [],
   "source": [
    "print(\"embedding matrix's shape: \", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3P9dVdI5l4q"
   },
   "outputs": [],
   "source": [
    "X = train_padded_sequences\n",
    "y = train[CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "YjhnqUqAEAbn",
    "outputId": "4d205c90-87a5-45c9-8e48-a2b763920f05"
   },
   "outputs": [],
   "source": [
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nKiX5cDSArk"
   },
   "outputs": [],
   "source": [
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.1, \n",
    "                              patience=2, \n",
    "                              min_lr=0.00001,\n",
    "                              mode='auto')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"BiLSTM_GLOVE_840B_300D.h5\",\n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True, \n",
    "                             mode='min',  \n",
    "                             verbose=2)\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "callbacks_list = [checkpoint, lr_schedule, early_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "VARBISPJ5lvZ",
    "outputId": "674b7b96-1563-48fb-9c7b-1e7327e2bd6f"
   },
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size+1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix], trainable=False, input_shape=(MAX_SEQUENCE_LENGTH,)))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences=True, dropout=0.25)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "history = model.fit(X, y, epochs=10, batch_size=64, validation_split=0.1, shuffle=False, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Loss and Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "U8EFf4ZF6bJy",
    "outputId": "74004dcb-8464-4a96-cfe6-6fa65dd99d9d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the submission.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "p70XfhRO6a_g",
    "outputId": "9a26c284-0916-48f7-fd22-8f13f93b6579"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_padded_sequences, batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission[CLASSES] = y_pred\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard scores \n",
    "### Private: 0.98005  Public: 0.98181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Toxic-Comment-Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
